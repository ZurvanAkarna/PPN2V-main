{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fcbe76",
   "metadata": {},
   "source": [
    "# N2V Denoising — Single 128×128 TEM Image\n",
    "\n",
    "Uses the **original PPN2V library** with minimal config changes.\n",
    "\n",
    "### What we change from defaults (and why)\n",
    "\n",
    "| Parameter | Default | Ours | Reason |\n",
    "|---|---|---|---|\n",
    "| `num_classes` | 800 (PN2V) | **1** | N2V mode: single mean output, no noise model |\n",
    "| `start_filts` | 64 | **32** | 128×128 = 16K pixels. 64 overfits |\n",
    "| `depth` | 5 | **3** | 2 pooling ops, bottleneck 16×16 for 64-px patches |\n",
    "| `merge_mode` | `'add'` | **`'concat'`** | Standard U-Net skip connections |\n",
    "| `patchSize` | 100 | **64** | Must be < 128 for random crop diversity |\n",
    "| `learningRate` | 1e-4 | **3e-4** | Faster convergence for small data |\n",
    "| `virtualBatchSize` | 20 | **1** | No gradient accumulation needed |\n",
    "| `noiseModel` | varies | **None** | Activates N2V mode automatically |\n",
    "\n",
    "### What stays exactly as-is from the original code\n",
    "- **Masking**: neighbor-replacement from 5×5 ROI (original N2V paper)\n",
    "- **Mask ratio**: ~3% (`patchSize²/32`)\n",
    "- **Weight init**: Xavier (UNet default)\n",
    "- **Loss**: `lossFunctionN2V` — MSE on masked pixels only\n",
    "- **Training functions**: `trainingPred()`, `lossFunction()` from `training.py`\n",
    "- **Prediction**: `prediction.predict()` from `prediction.py`\n",
    "- **Augmentation**: random flips + 90° rotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605677e",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1 — Mount Drive & Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# === EDIT THESE PATHS IF NEEDED ===\n",
    "DRIVE_DATA_PATH   = '/content/drive/MyDrive/PPN2V/DATASET_01'\n",
    "N2V_RESULTS_PATH  = '/content/drive/MyDrive/PPN2V/results/DATASET_01/n2v_optimized'\n",
    "PN2V_RESULTS_PATH = '/content/drive/MyDrive/PPN2V/results/DATASET_01/pn2v_bootstrap'\n",
    "COMPARISON_PATH   = '/content/drive/MyDrive/PPN2V/results/DATASET_01/comparison'\n",
    "\n",
    "for p in [N2V_RESULTS_PATH, PN2V_RESULTS_PATH, COMPARISON_PATH]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "print('Drive mounted')\n",
    "print(f'Data:    {DRIVE_DATA_PATH}')\n",
    "print(f'Results: {N2V_RESULTS_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa618e6",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2 — Clone Repo & Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a080f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, shutil\n",
    "\n",
    "REPO_PATH    = '/content/PPN2V'\n",
    "PROJECT_ROOT = '/content/PPN2V/PPN2V-main'\n",
    "GITHUB_REPO  = 'https://github.com/ZurvanAkarna/PPN2V-main.git'\n",
    "\n",
    "# 1. Always start fresh to avoid stale cache issues\n",
    "if os.path.exists(REPO_PATH):\n",
    "    shutil.rmtree(REPO_PATH)\n",
    "    print(f'Removed old {REPO_PATH}')\n",
    "\n",
    "# 2. Clone\n",
    "subprocess.run(['git', 'clone', GITHUB_REPO, REPO_PATH], check=True)\n",
    "print(f'Cloned to {REPO_PATH}')\n",
    "\n",
    "# 3. Verify structure\n",
    "print(f'\\nFiles in PROJECT_ROOT ({PROJECT_ROOT}):')\n",
    "for f in sorted(os.listdir(PROJECT_ROOT)):\n",
    "    print(f'  {f}')\n",
    "\n",
    "# 4. Install — capture output so errors are visible\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "def pip_install(args, label=''):\n",
    "    \"\"\"Run pip and show full output on failure.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, '-m', 'pip', 'install'] + args,\n",
    "        capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f'\\n❌ pip install {\" \".join(args)} FAILED:')\n",
    "        print(result.stdout)\n",
    "        print(result.stderr)\n",
    "        raise RuntimeError(f'pip install failed: {\" \".join(args)}')\n",
    "    else:\n",
    "        if label:\n",
    "            print(f'  ✓ {label}')\n",
    "\n",
    "pip_install(['hatchling'], 'hatchling')\n",
    "pip_install(['-e', '.'], 'ppn2v (editable)')\n",
    "pip_install(['tifffile', 'scikit-image'], 'tifffile + scikit-image')\n",
    "print('\\n✅ Installation complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d1894",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3 — Imports & GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile\n",
    "import time\n",
    "\n",
    "sys.path.insert(0, '/content/PPN2V/PPN2V-main/src')\n",
    "from ppn2v.unet.model import UNet\n",
    "from ppn2v.pn2v import training, prediction, utils\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabfdb11",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4 — Load Data\n",
    "\n",
    "**Edit `NOISY_FILENAME` and `GT_FILENAME` below.**  \n",
    "Both should be 128×128. The ground truth is **only** for evaluation (PSNR), never for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bfd1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in data directory\n",
    "print(f'Files in {DRIVE_DATA_PATH}:')\n",
    "for f in sorted(os.listdir(DRIVE_DATA_PATH)):\n",
    "    fpath = os.path.join(DRIVE_DATA_PATH, f)\n",
    "    if os.path.isfile(fpath):\n",
    "        print(f'  {f}  ({os.path.getsize(fpath)/1e3:.1f} KB)')\n",
    "\n",
    "# ============================================================\n",
    "# >>> EDIT THESE TWO FILENAMES <<<\n",
    "# ============================================================\n",
    "NOISY_FILENAME = 'noisy_image_jitter_skips_0__0_3_flags_0__0_4_Gaussian_0.6.tif'         # your noisy 128x128 image\n",
    "GT_FILENAME    = 'clean_image.tif'   # your clean ground truth\n",
    "# ============================================================\n",
    "\n",
    "noisy_img = tifffile.imread(os.path.join(DRIVE_DATA_PATH, NOISY_FILENAME)).astype(np.float32)\n",
    "gt_img    = tifffile.imread(os.path.join(DRIVE_DATA_PATH, GT_FILENAME)).astype(np.float32)\n",
    "\n",
    "# Handle 3D (1, H, W) files\n",
    "if noisy_img.ndim == 3: noisy_img = noisy_img[0]\n",
    "if gt_img.ndim == 3:    gt_img = gt_img[0]\n",
    "\n",
    "print(f'\\nNoisy: shape={noisy_img.shape}, range=[{noisy_img.min():.1f}, {noisy_img.max():.1f}]')\n",
    "print(f'GT:    shape={gt_img.shape}, range=[{gt_img.min():.1f}, {gt_img.max():.1f}]')\n",
    "\n",
    "# trainNetwork expects 3D array (N, H, W). N=1 for single image.\n",
    "# We use the SAME image for train & val. With only 128x128 pixels,\n",
    "# a spatial split leaves regions too small for 64x64 patches.\n",
    "# Blind-spot masking already prevents trivial identity solutions.\n",
    "train_data = noisy_img[np.newaxis, ...].copy()  # (1, 128, 128)\n",
    "val_data   = train_data.copy()\n",
    "\n",
    "print(f'\\nTrain: {train_data.shape},  Val: {val_data.shape}')\n",
    "\n",
    "# Baseline PSNR (noisy vs GT)\n",
    "data_range = gt_img.max() - gt_img.min()\n",
    "input_psnr = utils.PSNR(gt_img, noisy_img, range_=data_range)\n",
    "print(f'Input PSNR (noisy vs GT): {input_psnr:.2f} dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ddfd0a",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5 — Create U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ce7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(\n",
    "    num_classes=1,           # N2V: single output (mean prediction)\n",
    "    in_channels=1,           # grayscale\n",
    "    depth=3,                 # 2 pooling ops -> 32 -> 64 -> 128\n",
    "    start_filts=32,          # C=32\n",
    "    up_mode='transpose',\n",
    "    merge_mode='concat',     # standard U-Net concat skip connections\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(f'U-Net: {total_params:,} parameters')\n",
    "print(f'Encoder: 1 -> 32 -> 64 -> 128')\n",
    "print(f'Decoder: 128 -> 64 -> 32 -> 1')\n",
    "\n",
    "# Sanity check\n",
    "with torch.no_grad():\n",
    "    t = torch.randn(1, 1, 64, 64)\n",
    "    print(f'Forward: {list(t.shape)} -> {list(net(t).shape)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150495d0",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6 — Train with Per-Epoch PSNR Tracking\n",
    "\n",
    "This is the only substantial addition: after each epoch we call the original\n",
    "`prediction.predict()` on the full image and compute PSNR vs ground truth.\n",
    "\n",
    "The **training itself** uses the exact original functions:\n",
    "- `training.trainingPred()` — assembles batch, crops, masks, forward pass\n",
    "- `training.lossFunction()` — MSE on masked pixels / std²\n",
    "- `prediction.predict()` — standard forward with normalization + denormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07836e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAINING HYPERPARAMETERS ===\n",
    "PATCH_SIZE = 64\n",
    "NUM_MASKED = int(PATCH_SIZE**2 / 32.0)   # ~128 pixels = ~3.1%\n",
    "NUM_EPOCHS = 300\n",
    "STEPS_PER_EPOCH = 50\n",
    "BATCH_SIZE = 4\n",
    "LR = 3e-4\n",
    "\n",
    "print(f'Patch:   {PATCH_SIZE}x{PATCH_SIZE}')\n",
    "print(f'Masked:  {NUM_MASKED} pixels/patch ({NUM_MASKED/(PATCH_SIZE**2)*100:.1f}%)')\n",
    "print(f'Method:  neighbor replacement (original training.py)')\n",
    "print(f'Loss:    lossFunctionN2V (MSE on masked pixels only)')\n",
    "print(f'LR:      {LR},  Epochs: {NUM_EPOCHS}')\n",
    "\n",
    "# --- Normalization (same as trainNetwork line 340-341) ---\n",
    "combined = np.concatenate([train_data, val_data])\n",
    "net.mean = np.mean(combined)\n",
    "net.std  = np.std(combined)\n",
    "net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
    "\n",
    "train_losses = []\n",
    "val_losses   = []\n",
    "psnr_history = []\n",
    "best_val     = float('inf')\n",
    "best_psnr    = 0.0\n",
    "best_epoch   = 0\n",
    "dataCounter  = 0\n",
    "\n",
    "print(f'\\nNormalization: mean={net.mean:.2f}, std={net.std:.2f}')\n",
    "print(f'GT data range for PSNR: {data_range:.1f}')\n",
    "print(f'\\n{\"=\"*65}')\n",
    "print(f'{\"Epoch\":>5} | {\"Train\":>10} | {\"Val\":>10} | {\"PSNR(dB)\":>9} | {\"LR\":>9} | Note')\n",
    "print(f'{\"=\"*65}')\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    # ===== TRAIN (original functions) =====\n",
    "    net.train()\n",
    "    ep_losses = []\n",
    "    for step in range(STEPS_PER_EPOCH):\n",
    "        optimizer.zero_grad()\n",
    "        outputs, labels, masks, dataCounter = training.trainingPred(\n",
    "            train_data, net, dataCounter,\n",
    "            PATCH_SIZE, BATCH_SIZE, NUM_MASKED,\n",
    "            device, augment=True, supervised=False)\n",
    "        loss = training.lossFunction(\n",
    "            outputs, labels, masks,\n",
    "            noiseModel=None, pn2v=False, std=net.std)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ep_losses.append(loss.item())\n",
    "\n",
    "    avg_train = np.mean(ep_losses)\n",
    "    train_losses.append(avg_train)\n",
    "\n",
    "    # ===== VALIDATION (original functions) =====\n",
    "    net.eval()\n",
    "    v_losses = []\n",
    "    valCounter = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(20):\n",
    "            outputs, labels, masks, valCounter = training.trainingPred(\n",
    "                val_data, net, valCounter,\n",
    "                PATCH_SIZE, BATCH_SIZE, NUM_MASKED,\n",
    "                device, augment=False, supervised=False)\n",
    "            loss = training.lossFunction(\n",
    "                outputs, labels, masks,\n",
    "                noiseModel=None, pn2v=False, std=net.std)\n",
    "            v_losses.append(loss.item())\n",
    "\n",
    "    avg_val = np.mean(v_losses)\n",
    "    val_losses.append(avg_val)\n",
    "\n",
    "    # ===== PSNR (original prediction.predict) =====\n",
    "    with torch.no_grad():\n",
    "        denoised, _ = prediction.predict(\n",
    "            noisy_img, net, noiseModel=None,\n",
    "            device=device, outScaling=10.0)\n",
    "    psnr = utils.PSNR(gt_img, denoised, range_=data_range)\n",
    "    psnr_history.append(psnr)\n",
    "\n",
    "    # ===== CHECKPOINT =====\n",
    "    note = ''\n",
    "    if avg_val < best_val:\n",
    "        best_val = avg_val\n",
    "        best_epoch = epoch + 1\n",
    "        best_psnr = psnr\n",
    "        torch.save(net, os.path.join(N2V_RESULTS_PATH, 'best_n2v.net'))\n",
    "        note = '<< BEST'\n",
    "    torch.save(net, os.path.join(N2V_RESULTS_PATH, 'last_n2v.net'))\n",
    "\n",
    "    scheduler.step(avg_val)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # ===== PRINT =====\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0 or note:\n",
    "        print(f'{epoch+1:5d} | {avg_train:10.5f} | {avg_val:10.5f} | '\n",
    "              f'{psnr:8.2f}  | {current_lr:.2e} | {note}')\n",
    "\n",
    "    # Save histories each epoch (safe against crashes)\n",
    "    np.savez(os.path.join(N2V_RESULTS_PATH, 'history.npz'),\n",
    "             train=train_losses, val=val_losses, psnr=psnr_history)\n",
    "\n",
    "total_min = (time.time() - t_start) / 60\n",
    "peak_epoch = int(np.argmax(psnr_history) + 1)\n",
    "peak_psnr  = max(psnr_history)\n",
    "\n",
    "print(f'{\"=\"*65}')\n",
    "print(f'Done in {total_min:.1f} min')\n",
    "print(f'Best val loss:  epoch {best_epoch}  (PSNR {best_psnr:.2f} dB)')\n",
    "print(f'Peak PSNR:      epoch {peak_epoch}  ({peak_psnr:.2f} dB)')\n",
    "print(f'Input PSNR:     {input_psnr:.2f} dB')\n",
    "print(f'Improvement:    +{peak_psnr - input_psnr:.2f} dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3033833c",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7 — Loss vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f58b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_arr = np.arange(1, len(train_losses) + 1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(epochs_arr, train_losses, 'b-', alpha=0.7, label='Train')\n",
    "ax1.plot(epochs_arr, val_losses,   'r-', alpha=0.7, label='Val')\n",
    "ax1.axvline(best_epoch, color='green', ls='--', alpha=0.5,\n",
    "            label=f'Best epoch {best_epoch}')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(epochs_arr, train_losses, 'b-', alpha=0.7, label='Train')\n",
    "ax2.plot(epochs_arr, val_losses,   'r-', alpha=0.7, label='Val')\n",
    "ax2.axvline(best_epoch, color='green', ls='--', alpha=0.5,\n",
    "            label=f'Best epoch {best_epoch}')\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Loss (log)')\n",
    "ax2.set_title('Loss (log scale)'); ax2.set_yscale('log')\n",
    "ax2.legend(); ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(N2V_RESULTS_PATH, 'loss_vs_epoch.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252dfbf",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8 — PSNR vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(epochs_arr, psnr_history, 'b-', lw=1.5,\n",
    "        label='PSNR (denoised vs GT)')\n",
    "ax.axhline(input_psnr, color='red', ls='--', alpha=0.7,\n",
    "           label=f'Input PSNR = {input_psnr:.2f} dB')\n",
    "ax.scatter([peak_epoch], [peak_psnr], color='green', s=100, zorder=5,\n",
    "           label=f'Peak: {peak_psnr:.2f} dB @ epoch {peak_epoch}')\n",
    "ax.axvline(peak_epoch, color='green', ls='--', alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "ax.set_title('PSNR vs Epoch', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(N2V_RESULTS_PATH, 'psnr_vs_epoch.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Peak PSNR:  {peak_psnr:.2f} dB  (epoch {peak_epoch})')\n",
    "print(f'Input PSNR: {input_psnr:.2f} dB')\n",
    "print(f'Gain:       +{peak_psnr - input_psnr:.2f} dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3921128",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9 — Load Best & Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e51d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "best_net = torch.load(\n",
    "    os.path.join(N2V_RESULTS_PATH, 'best_n2v.net'),\n",
    "    map_location=device)\n",
    "best_net.eval()\n",
    "\n",
    "# Predict using original prediction.predict()\n",
    "with torch.no_grad():\n",
    "    n2v_denoised, _ = prediction.predict(\n",
    "        noisy_img, best_net, noiseModel=None,\n",
    "        device=device, outScaling=10.0)\n",
    "\n",
    "final_psnr = utils.PSNR(gt_img, n2v_denoised, range_=data_range)\n",
    "print(f'Denoised PSNR: {final_psnr:.2f} dB')\n",
    "print(f'Input PSNR:    {input_psnr:.2f} dB')\n",
    "print(f'Improvement:   +{final_psnr - input_psnr:.2f} dB')\n",
    "\n",
    "# Save\n",
    "tifffile.imwrite(os.path.join(N2V_RESULTS_PATH, 'n2v_denoised.tif'),\n",
    "                 n2v_denoised.astype(np.float32))\n",
    "tifffile.imwrite(os.path.join(N2V_RESULTS_PATH, 'original_noisy.tif'),\n",
    "                 noisy_img.astype(np.float32))\n",
    "print(f'\\nSaved to {N2V_RESULTS_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999f2c3",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10 — Visual Comparison: Noisy / Denoised / GT / Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b53fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = np.percentile(gt_img, [1, 99])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "axes[0,0].imshow(noisy_img, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "axes[0,0].set_title(f'Noisy Input\\nPSNR = {input_psnr:.2f} dB', fontsize=13)\n",
    "\n",
    "axes[0,1].imshow(n2v_denoised, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "axes[0,1].set_title(f'N2V Denoised (Best Checkpoint)\\nPSNR = {final_psnr:.2f} dB', fontsize=13)\n",
    "\n",
    "axes[1,0].imshow(gt_img, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "axes[1,0].set_title('Ground Truth', fontsize=13)\n",
    "\n",
    "residual = noisy_img - n2v_denoised\n",
    "rlim = np.std(residual) * 3\n",
    "axes[1,1].imshow(residual, cmap='RdBu_r', vmin=-rlim, vmax=rlim)\n",
    "axes[1,1].set_title(f'Residual (Removed Noise)\\nstd = {np.std(residual):.1f}', fontsize=13)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('N2V Single-Image Denoising', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(N2V_RESULTS_PATH, 'comparison_4panel.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0ff56",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11 — Error Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a526f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "err_noisy    = np.abs(noisy_img - gt_img)\n",
    "err_denoised = np.abs(n2v_denoised - gt_img)\n",
    "err_max      = max(err_noisy.max(), err_denoised.max())\n",
    "\n",
    "axes[0].imshow(err_noisy, cmap='hot', vmin=0, vmax=err_max)\n",
    "axes[0].set_title(f'|Noisy \\u2212 GT|\\nMAE = {err_noisy.mean():.2f}', fontsize=13)\n",
    "\n",
    "axes[1].imshow(err_denoised, cmap='hot', vmin=0, vmax=err_max)\n",
    "axes[1].set_title(f'|Denoised \\u2212 GT|\\nMAE = {err_denoised.mean():.2f}', fontsize=13)\n",
    "\n",
    "improvement = err_noisy - err_denoised\n",
    "ilim = np.std(improvement) * 3\n",
    "axes[2].imshow(improvement, cmap='RdYlGn', vmin=-ilim, vmax=ilim)\n",
    "axes[2].set_title('Improvement Map\\n(green = denoised better)', fontsize=13)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(N2V_RESULTS_PATH, 'error_maps.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a34984",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12 — Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('  N2V RESULTS SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'''\n",
    "  Architecture\n",
    "    Model:          UNet (depth=3, C=32, concat skips)\n",
    "    Parameters:     {total_params:,}\n",
    "    Init:           Xavier (original default)\n",
    "    Activations:    ReLU  (original default)\n",
    "\n",
    "  Training (all using original training.py functions)\n",
    "    Masking:        neighbor replacement ~3%\n",
    "    Patch size:     {PATCH_SIZE}x{PATCH_SIZE}\n",
    "    Learning rate:  {LR}\n",
    "    Batch size:     {BATCH_SIZE}\n",
    "    Epochs run:     {len(train_losses)} / {NUM_EPOCHS}\n",
    "\n",
    "  Results\n",
    "    Input PSNR:     {input_psnr:.2f} dB\n",
    "    Best PSNR:      {peak_psnr:.2f} dB  (epoch {peak_epoch})\n",
    "    Final PSNR:     {final_psnr:.2f} dB  (best checkpoint, epoch {best_epoch})\n",
    "    Improvement:    +{final_psnr - input_psnr:.2f} dB\n",
    "\n",
    "  Files saved to {N2V_RESULTS_PATH}:\n",
    "''')\n",
    "for f in sorted(os.listdir(N2V_RESULTS_PATH)):\n",
    "    sz = os.path.getsize(os.path.join(N2V_RESULTS_PATH, f)) / 1e3\n",
    "    print(f'    {f:35s} ({sz:.1f} KB)')\n",
    "\n",
    "print(f'\\n  Next: use n2v_denoised.tif as pseudo-GT for PN2V bootstrap')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
